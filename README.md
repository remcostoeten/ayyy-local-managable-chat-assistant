# ğŸš€ AI-Powered Knowledge Base Platform

[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=flat-square&logo=typescript&logoColor=white)]()
[![Next.js 14](https://img.shields.io/badge/Next.js_14-black?style=flat-square&logo=next.js&logoColor=white)]()
[![Turso](https://img.shields.io/badge/Turso-00ADD8?style=flat-square&logo=sqlite&logoColor=white)]()
[![RAG](https://img.shields.io/badge/RAG-ğŸ”¥-yellow?style=flat-square)]()

> ğŸŒŸ **Supercharge your knowledge base with AI!** Built with Next.js 14, Turso, and a dash of magic âœ¨

## ğŸ¯ What Makes This Awesome?

### ğŸ¨ Stunning Admin Dashboard
- **Drop-dead Gorgeous UI**: Powered by shadcn/ui + Tailwind CSS
- **Theme Paradise**: 
  - ğŸ¨ Catpuccini vibes
  - âš¡ Supabase sleekness
  - â˜• Caffeine kick
  - ğŸŒ™ Night Bourbon elegance
- **Smooth Animations**: Framer Motion goodness
- **Dark/Light Magic**: Seamless theme switching

### ğŸ§  AI Superpowers
- **Smart Document Processing**: Let AI do the heavy lifting
- **Vector Magic**: Transform documents into searchable knowledge
- **RAG Integration**: Context-aware responses that make sense
- **Multi-Model Support**: Choose your AI champion
  
  #### ğŸ–¥ï¸ Local Models (via Ollama)
  - ğŸš€ **Recommended Models**:
    - Mistral 7B (4.1GB): Fast responses, excellent Dutch support
    - Neural-Chat (4.8GB): Optimized for chat interactions
  
  - ğŸ”„ **Alternative Models**:
    - Llama 2 7B (3.8GB): Meta's latest model, good all-rounder
    - Llama 2 13B (7.3GB): Larger, more capable version
  
  - ğŸª¶ **Lightweight Options**:
    - TinyLlama (1.2GB): Ultra lightweight, perfect for testing
    - Phi-2 (1.7GB): Microsoft's small but mighty model
  
  #### â˜ï¸ Cloud Integration
  - Groq API integration available in admin interface
  - Free credits for new users
  - Configure your API key in settings

### âš¡ Lightning Fast with Turso
- **Edge-Ready Database**: Powered by libSQL
- **Global Distribution**: Your data, everywhere
- **Blazing Performance**: Built for speed
- **Rock-Solid Foundation**: Enterprise-grade reliability

## ğŸš€ Quick Start

```bash
# Clone this beauty
git clone https://github.com/yourusername/ai-kb
cd ai-kb

# Install dependencies
pnpm install

# Install AI models (interactive)
pnpm run install-models

# Light it up! ğŸ”¥
pnpm dev
```

### ğŸ“¦ Model Installation

The `install-models` script provides an interactive installation experience:

1. View detailed information about available models
2. Select which models to install (space to select, return to confirm)
3. Models are grouped by category:
   - Recommended: Best for most use cases
   - Alternative: Additional powerful options
   - Lightweight: Perfect for testing
4. See size requirements before downloading

> ğŸ’¡ **Note**: Local models require disk space (1.2GB - 7.3GB each). For testing, start with lightweight options like TinyLlama (1.2GB) or Phi-2 (1.7GB).

> ğŸ’¡ **Cloud Models**: Groq integration is configured separately in the admin interface under Settings.

## ğŸ’ª Tech Stack of Champions

- **Frontend**: 
  - ğŸ”¥ Next.js 14 (App Router)
  - âš›ï¸ React (Latest and Greatest)
  - ğŸ“˜ TypeScript (Type Safety FTW)
  
- **Styling**: 
  - ğŸ¨ Tailwind CSS
  - ğŸ­ shadcn/ui (Beautiful Components)
  - âœ¨ Framer Motion (Smooth Animations)
  
- **Database & AI**: 
  - ğŸš€ Turso (Edge Database)
  - ğŸ§  Vector Embeddings (Coming Soon)
  - ğŸ¤– RAG Integration (In Progress)
  - ğŸ¯ Local AI Models via Ollama

- **Development**: 
  - ğŸ“¦ pnpm (Fast & Efficient)
  - âš¡ Bun (Speed Demon)
  - ğŸ› ï¸ DrizzleORM (Type-Safe Queries)

## ğŸŒˆ Features in the Pipeline

Get hyped for what's coming:
- ğŸ” Advanced semantic search
- ğŸ“Š Real-time analytics dashboard
- ğŸ¤– Custom AI model integration
- ğŸ” Enterprise-grade security
- ğŸ“± Mobile app companion

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    A[Your Content] -->|Magic| B[AI Processor]
    B -->|Vector Embedding| C[Turso DB]
    C -->|Lightning Fast| D[Knowledge Base]
    D -->|Smart Retrieval| E[RAG Engine]
    E -->|AI Enhanced| F[Responses]
```

## ğŸš€ Performance

Built for speed and scale with:
- âš¡ Edge-ready architecture
- ğŸŒ Global data distribution
- ğŸš„ Optimized queries
- ğŸ¯ Smart caching

## ğŸŒŸ Coming Soon

We're cooking up some incredible features:
- ğŸ“± Mobile app
- ğŸ”„ Real-time collaboration
- ğŸ¯ Custom AI fine-tuning
- ğŸŒ API marketplace

## ğŸ› ï¸ Development Status

- âœ… Admin Dashboard
- âœ… Theme System
- âœ… Turso Integration
- âœ… Model Management
- ğŸ—ï¸ RAG Implementation
- ğŸ—ï¸ Vector Search
- ğŸ¯ Analytics (Planned)

---

<p align="center">
  <strong>Built with â¤ï¸ by [remco stoeten](https://github.com/remcostoeten)</strong>
</p>

## Additional information

### Models & Integration

#### Local Models (via Ollama)
The app supports various local models through Ollama:
- **Recommended**:
  - Mistral 7B: Fast responses, excellent Dutch support (4.1GB)
  - Neural-Chat: Optimized for chat interactions (4.8GB)
- **Alternative**:
  - Llama 2 models: Available in 7B and 13B versions
- **Lightweight**:
  - TinyLlama and Phi-2: Perfect for testing (1.2-1.7GB)

#### Cloud Integration
For users who prefer cloud-based inference:
- Configure Groq API key in the admin interface
- Get free credits when signing up
- No additional setup required